{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "### Shan Shan Bianca Tan a1909709"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9649d",
   "metadata": {},
   "source": [
    "## A. Information Retrieval system (1 person work)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f81f12a-6e48-409b-a683-681c509537ec",
   "metadata": {},
   "source": [
    "## 1. Reading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b035ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering,pipeline\n",
    "import string\n",
    "\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24aa6d-0dec-4006-86de-a1b076e071f6",
   "metadata": {},
   "source": [
    "### 1.1 Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5dfebd-c6b8-4cf0-b28d-c16eaed6f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset\n",
    "file = 'news_dataset.csv'\n",
    "df = pd.read_csv(file, encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b93ab1-f72a-4282-9a4c-1f8b4e092f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reduce dataframe size\n",
    "def reduce_df(df, sample_size=None):\n",
    "    if sample_size:\n",
    "        df = df.sample(n=sample_size, random_state=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b3f4ec-3977-47bf-bbb7-e2b9f8ce1ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf = reduce_df(df, 150) #  Take 150 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d99ddf-8ad3-4801-9396-9dff7a762a8f",
   "metadata": {},
   "source": [
    "### 1.2 Reading question and answer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ae7706-5682-4837-b2f5-517a1b06f22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17574, 'Who is the vice chairman of Samsung?', 'Jay Y. Lee'),\n",
       " (17298,\n",
       "  'Which subway is opening in New York City on Sunday?',\n",
       "  'Second Avenue subway'),\n",
       " (17339, 'What amount did Fox News offer?', '20 Million'),\n",
       " (17300, \"Who is Mr. Roof's lead lawyer?\", 'David I. Bruck')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_qa = pd.read_csv('test_questions_and_answers.csv', encoding = 'latin1')\n",
    "\n",
    "test_questions_and_answers = []\n",
    "for row in test_qa.itertuples():\n",
    "    # Format questions to be in necesssary format\n",
    "    test_questions_and_answers.append((row[1], row[2], row[3]))\n",
    "\n",
    "# Display top questions to ensure format is correct\n",
    "test_questions_and_answers[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba7b31-d463-4f74-b11b-f82e90da8747",
   "metadata": {},
   "source": [
    "## 2. Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9d9f4-b10c-4236-8b0f-c0d9b522a445",
   "metadata": {},
   "source": [
    "Data pre-processing is done to ensure that the data fed to the model is of high quality. This ensures that the model can efficiently extract the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f701ca30-e0fa-444d-ba03-e66e3865983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from (Germec, 2023)\n",
    "# Function to preprocess the text for better results\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def preprocess(text):\n",
    "    # Lower casing\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replacing \"?\" with blank\n",
    "    text = text.replace(\"?\", \"\")   \n",
    "    \n",
    "    # Lemmatization\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "# Function to split an article into individual sentences\n",
    "def split_article (article):\n",
    "    test = article.replace(\"?\", \"\")\n",
    "    #   Split article into sentences\n",
    "    sentences = test.split(\".\")\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "134c281b-a7b2-4028-be12-424d0721387f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  One night nearly 140 years ago, Samuel Clemens told his young daughters Clara and Susie a bedtime story about a poor boy who eats a magic flower that gives him the ability to talk to animals. Storytelling was a nightly ritual in the Clemens home. But something about this particular tale must have stuck with Clemens, better known as Mark Twain, because he decided to jot down some notes about it. The story might have ended there, lost to history. But decades later, the scholar John Bird was searching the Twain archives at the University of California, Berkeley, when he came across the notes for the story, which Twain titled ?Oleomargarine. ? Mr. Bird was astonished to find a richly imagined fable, in Twain?s inimitable voice. He and other scholars believe it may be the only written remnant of a children?s fairy tale from Twain, though he told his daughters stories constantly. It?s impossible to know why Twain did not finish the tale, or if he ever intended it for a wider audience. Now, m\n",
      "\n",
      "Processed:  one night nearly 140 year ago , samuel clemen tell his young daughter clara and susie a bedtime story about a poor boy who eat a magic flower that give he the ability to talk to animal . storytelle be a nightly ritual in the clemen home . but something about this particular tale must have stick with clemen , well know as mark twain , because he decide to jot down some note about it . the story might have end there , lose to history . but decade later , the scholar john bird be search the twain archive at the university of california , berkeley , when he come across the note for the story , which twain title oleomargarine .   mr . bird be astonish to find a richly imagine fable , in twain inimitable voice . he and other scholar believe it may be the only write remnant of a children fairy tale from twain , though he tell his daughter story constantly . its impossible to know why twain do not finish the tale , or if he ever intend it for a wide audience . now , more than a century after t\n"
     ]
    }
   ],
   "source": [
    "# Testing to ensure preprocess function is working\n",
    "test = rdf.article.iloc[2]\n",
    "print(\"Original: \", test[:1000]) # Print first 1000 characters to check\n",
    "print()\n",
    "print(\"Processed: \", preprocess(test)[:1000]) # Print first 1000 characters to check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b26f1bc-94c0-4a6e-aee3-53656b43b2ec",
   "metadata": {},
   "source": [
    "## 3 Selection of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dfdfb6-31d1-4f59-a843-2361e4d52e1c",
   "metadata": {},
   "source": [
    "Section 3.1 will discuss the hybrid model while section 3.2 will discuss the direct usage of a pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d313668-5e5d-4fcc-a2aa-7038e5c551a3",
   "metadata": {},
   "source": [
    "### 3.1 Hybrid Model (Self built functions + Pre-trained Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc6b1e-825b-4f74-ad3a-498e89ae9171",
   "metadata": {},
   "source": [
    "#### 3.1.1 Coreference Resolution Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61565401-8d91-4178-a864-23d3fe37567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Jessica is sick and he did not go to school.\n",
      "Resolved sentence: Jessica is sick and Jessica did not go to school.\n"
     ]
    }
   ],
   "source": [
    "# Code from Workshop\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def resolve_coreferences(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    entity_mentions = {}\n",
    "\n",
    "    # Iterate through named entities and store mentions\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\"]:\n",
    "            entity_mentions[ent.root.text] = ent.text\n",
    "\n",
    "    # Resolve pronouns to named entity mentions\n",
    "    resolved_tokens = []\n",
    "    corefs = {}  # Dictionary to store coreference relations\n",
    "\n",
    "    # Iterate over each word in the sentence\n",
    "    for i, token in enumerate(doc):\n",
    "        # If the word is a pronoun\n",
    "        if token.pos_ == 'PRON' and token.text.lower() in [\"he\", \"him\", \"she\", \"her\", \"it\", \"they\", \"them\",\"his\"]:\n",
    "            # Iterate over each token before the pronoun\n",
    "            for j in range(i - 1, -1, -1):\n",
    "                # If the token is a noun or a proper noun (part of named entity), it is a possible antecedent\n",
    "                if doc[j].pos_ in ['NOUN', 'PROPN']:\n",
    "                    # Save the antecedent as the coreference of the pronoun\n",
    "                    corefs[i] = j\n",
    "                    resolved_tokens.append(entity_mentions.get(doc[j].text, doc[j].text))\n",
    "                    resolved_tokens.append(' ')\n",
    "                    break\n",
    "            else:\n",
    "                # If no antecedent found, keep the pronoun as is\n",
    "                resolved_tokens.append(token.text_with_ws)\n",
    "        else:\n",
    "            resolved_tokens.append(token.text_with_ws)\n",
    "\n",
    "    # Join resolved tokens to form the resolved sentence\n",
    "    resolved_sentence = ''.join(resolved_tokens)\n",
    "    return resolved_sentence\n",
    "\n",
    "# Test Coreference Resolution on a simple sentence to ensure it works\n",
    "sentence = \"Jessica is sick and he did not go to school.\"\n",
    "resolved_sentence = resolve_coreferences(sentence)\n",
    "print(\"Original sentence:\", sentence)\n",
    "print(\"Resolved sentence:\", resolved_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb17b0b",
   "metadata": {},
   "source": [
    "#### 3.1.2 Extract Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20bb03e5-97fa-45ae-8b18-8a55d88fab5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9785180687904358, 'start': 0, 'end': 4, 'answer': 'Jack'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using pretrain model to obtain the most relevant answer from the phrase\n",
    "# In appendix, I have included attempts of using self-written functions for extraction of entities \n",
    "# but it does not provide accurate results\n",
    "# Code from (Chan et al. 2024) \n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering,pipeline\n",
    "model_name = \"deepset/tinyroberta-squad2\"\n",
    "\n",
    "def extract_most_relevant_entity(question, relevant_sentence):\n",
    "    nlp_extraction = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "\n",
    "    QA_input = {\n",
    "    'question': question,\n",
    "    'context': sentence}\n",
    "    \n",
    "    return nlp_extraction(QA_input)\n",
    "\n",
    "# Test extraction of relevant answer on a simple sentence to ensure it works\n",
    "question = 'Who fell down?'\n",
    "sentence = \"Jack fell down and John helped him.\"\n",
    "extract_most_relevant_entity(question, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035c033",
   "metadata": {},
   "source": [
    "#### 3.1.3 Text matching utility\n",
    "- Find most relevant sentence and its confidence score in the article based on the user question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "922431e0-b701-4a98-b1d5-e9c0b0b036b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Workshop\n",
    "# Text matching utility\n",
    "# Find most relevant sentence in article through computing similarity score between question and sentence\n",
    "def find_most_relevant_sentence(user_question, article_sentences):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(article_sentences + [user_question])\n",
    "    \n",
    "    # Calculate cosine similarity between the question and each sentence\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()\n",
    "    most_similar_sentence_index = cosine_similarities.argmax()\n",
    "    \n",
    "    return article_sentences[most_similar_sentence_index], cosine_similarities[most_similar_sentence_index]\n",
    "\n",
    "# To apply find_most_relevant_sentence\n",
    "def get_answer_with_relevant_sentence(sentences, question):\n",
    "    # Find the most relevant sentence\n",
    "    most_relevant_sentence, similarity_score = find_most_relevant_sentence(question, sentences)\n",
    "    return similarity_score,most_relevant_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d449f712",
   "metadata": {},
   "source": [
    "#### 3.1.4 Test utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c7713c5-8f07-48bf-af2f-effdb426bdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the vice chairman of Samsung? Jay Y. Lee\n",
      "{'score': 0.952017605304718, 'start': 1, 'end': 4, 'answer': 'lee'}\n",
      "\n",
      "Which subway is opening in New York City on Sunday? Second Avenue subway\n",
      "{'score': 0.5400621891021729, 'start': 5, 'end': 25, 'answer': 'second avenue subway'}\n",
      "\n",
      "What amount did Fox News offer? 20 Million\n",
      "{'score': 0.9534481763839722, 'start': 81, 'end': 93, 'answer': '$ 20 million'}\n",
      "\n",
      "Who is Mr. Roof's lead lawyer? David I. Bruck\n",
      "{'score': 0.917976438999176, 'start': 21, 'end': 28, 'answer': 'david i'}\n",
      "\n",
      "Who is the spokesman? Numan Kurtulmus\n",
      "{'score': 9.806988998661836e-09, 'start': 307, 'end': 316, 'answer': 'caliphate'}\n",
      "\n",
      "Where is the gunman from? Kyrgyzstan or elsewhere in Central Asia\n",
      "{'score': 9.353278151991162e-09, 'start': 153, 'end': 166, 'answer': 'islamic state'}\n",
      "\n",
      "Where is Megyn Kelly moving to from Fox News? NBC\n",
      "{'score': 4.120644714333821e-09, 'start': 59, 'end': 67, 'answer': 'fox news'}\n",
      "\n",
      "What salary was Megyn Kelly offered by the Murdoch family? More than $20 million a year\n",
      "{'score': 3.9487193134846166e-05, 'start': 32, 'end': 39, 'answer': '12 year'}\n",
      "\n",
      "How many students attend the Evergrande Football School? 2,800 students\n",
      "{'score': 0.7445366382598877, 'start': 123, 'end': 130, 'answer': '2 , 800'}\n",
      "\n",
      "What is the main newspaper of the Communist Party in China? People's Daily\n",
      "{'score': 0.9737105369567871, 'start': 1, 'end': 13, 'answer': 'people daily'}\n",
      "\n",
      "What rank did the national men's soccer team of China recently achieve in FIFA rankings? 83rd\n",
      "{'score': 1.894396405077714e-06, 'start': 38, 'end': 42, 'answer': '83rd'}\n",
      "\n",
      "What is the term used to describe the biggest players in the technology industry? sharks\n",
      "{'score': 1.4801122105723152e-08, 'start': 72, 'end': 78, 'answer': 'minnow'}\n",
      "\n",
      "Who are the \"Frightful Five\" mentioned in the article? Amazon, Apple, Facebook, Microsoft, and Alphabet (Google's parent company)\n",
      "{'score': 8.953780934461975e-07, 'start': 193, 'end': 260, 'answer': 'entertainment giant , cable and phone company , and the news medium'}\n",
      "\n",
      "What special item did Mr. Purcell surprise Ms. Bui with? Engagement ring\n",
      "{'score': 2.2059888638015224e-11, 'start': 1, 'end': 18, 'answer': 'purcell invite ms'}\n",
      "\n",
      "Which city did Mr. Purcell and Ms. Bui get engaged in? San Francisco\n",
      "{'score': 9.353802454814542e-12, 'start': 1, 'end': 19, 'answer': 'purcell invite ms '}\n",
      "\n",
      "What is Walter J. Clayton's background? Wall Street lawyer\n",
      "{'score': 0.00030984761542640626, 'start': 4, 'end': 22, 'answer': 'wall street lawyer'}\n",
      "\n",
      "What accessory did the author's mother, keep in her car? A bright yellow hard hat\n",
      "{'score': 0.4531517028808594, 'start': 25, 'end': 47, 'answer': 'bright yellow hard hat'}\n",
      "\n",
      "What country was chosen as the top destination for the 2017 list? Canada\n",
      "{'score': 4.668410658048572e-10, 'start': 1, 'end': 45, 'answer': 'my favorite destination do not make the list'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test text matching utility\n",
    "# Code by self\n",
    "predicted_answers = []\n",
    "for i in range(0, len(test_questions_and_answers)):\n",
    "    #   Get article id\n",
    "    testid = test_questions_and_answers[i][0]\n",
    "\n",
    "    #   Get sentences\n",
    "    for row in df.itertuples():\n",
    "        if row.id == testid:\n",
    "            current_article = preprocess(row.article)\n",
    "            #print(current_article)\n",
    "            current_article = resolve_coreferences(current_article)\n",
    "            sentences = split_article(current_article)\n",
    "    # Obtain the most relevant sentence\n",
    "    score, sentence = get_answer_with_relevant_sentence(sentences, test_questions_and_answers[i][1])\n",
    "    print(test_questions_and_answers[i][1], test_questions_and_answers[i][2])\n",
    "\n",
    "    # Obtain the most relevant entity\n",
    "    answer = extract_most_relevant_entity(test_questions_and_answers[i][1], sentence)\n",
    "    predicted_answers.append(answer['answer'])\n",
    "    print(answer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb0373-1d25-4716-b181-038a192935d7",
   "metadata": {},
   "source": [
    "#### 3.1.5 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b26645-b70d-425e-a537-92494dcaee25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jay Y. Lee',\n",
       " 'Second Avenue subway',\n",
       " '20 Million',\n",
       " 'David I. Bruck',\n",
       " 'Numan Kurtulmus',\n",
       " 'Kyrgyzstan or elsewhere in Central Asia',\n",
       " 'NBC',\n",
       " 'More than $20 million a year',\n",
       " '2,800 students',\n",
       " \"People's Daily\",\n",
       " '83rd',\n",
       " 'sharks',\n",
       " \"Amazon, Apple, Facebook, Microsoft, and Alphabet (Google's parent company)\",\n",
       " 'Engagement ring',\n",
       " 'San Francisco',\n",
       " 'Wall Street lawyer',\n",
       " 'A bright yellow hard hat',\n",
       " 'Canada']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtaining list of true answers\n",
    "true_answers = [i[2] for i in test_questions_and_answers]\n",
    "true_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bab5f733-f83a-4acc-85ac-78270c763039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) Score: 0.17\n"
     ]
    }
   ],
   "source": [
    "# Code by self\n",
    "# Function to compute exact match score\n",
    "def calculate_exact_match(predicted_answers, true_answers):\n",
    "    # Initialize the EM score\n",
    "    em_score = 0.0\n",
    "    \n",
    "    # Iterate through each pair of predicted and true answers\n",
    "    for pred, true in zip(predicted_answers, true_answers):\n",
    "        # Check if the predicted answer exactly matches the true answer (lowercasing of both)\n",
    "        if pred.lower() == true.lower():\n",
    "            em_score += 1\n",
    "    \n",
    "    # Calculate the EM score as the proportion of exact matches\n",
    "    em_score /= len(predicted_answers)\n",
    "    \n",
    "    return em_score\n",
    "\n",
    "# Call calculate_exact_match function \n",
    "em_score = calculate_exact_match(predicted_answers, true_answers)\n",
    "print(\"Exact Match (EM) Score:\", round(em_score,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68facfd-1b91-4748-aafd-54ba8caf2d59",
   "metadata": {},
   "source": [
    "### 3.2 Pre-train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20c09a-4ce9-45b8-8d6d-2fdaea8f0f29",
   "metadata": {},
   "source": [
    "In comparison to the model above, it can be seen that the answers from the pre trained model makes much more sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80869008-684e-4509-afa8-c7af2988b3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.991371750831604,\n",
       " 'start': 70,\n",
       " 'end': 85,\n",
       " 'answer': 'clara and susie'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code from (Chan et al. 2024) \n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/tinyroberta-squad2\"\n",
    "\n",
    "# Testing on preprocessed test\n",
    "nlp_qa = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Who are the daughters?',\n",
    "    'context':  preprocess(test)}\n",
    "res = nlp_qa(QA_input)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "300969a8-c638-4930-9ba1-ca65088e6cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/question_answering.py:326: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Code from (Chan et al. 2024) \n",
    "def answer_questions(test_questions_and_answers, df, k=5, print_output = 'N'):\n",
    "    # Pre-trained Model\n",
    "    # test_questions_and_answers (list): List of tuples containing test question IDs, questions, and expected answers\n",
    "    # df (DataFrame): DataFrame containing articles and their corresponding IDs\n",
    "    # k (int): Number of top answers to retrieve\n",
    "    # print_output (str): Whether to print the output (default is 'N')\n",
    "\n",
    "    model_name = \"deepset/tinyroberta-squad2\"\n",
    "    nlp_qa = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "    predicted_answers = []\n",
    "    for i in range(len(test_questions_and_answers)):\n",
    "        # Get article id\n",
    "        testid = test_questions_and_answers[i][0]\n",
    "\n",
    "        # Get context\n",
    "        for row in df.itertuples():\n",
    "            if row.id == testid:\n",
    "                context = row.article\n",
    "\n",
    "        # Preprocess text (Cleaning/Lemmatization/Lowercasing)\n",
    "        context = preprocess(context)\n",
    "\n",
    "        # If print_output == 'Y', print question and expected answer\n",
    "        qn = test_questions_and_answers[i][1]\n",
    "        if print_output == 'Y':\n",
    "            print(\"Question:\", qn)\n",
    "        expected_ans = test_questions_and_answers[i][2]\n",
    "        if print_output == 'Y':\n",
    "            print('Expected answer:     ', expected_ans, '\\n')\n",
    "\n",
    "        qna = {\n",
    "            'question': qn,\n",
    "            'context': context}\n",
    "\n",
    "        # Answer the question\n",
    "        answers = nlp_qa(qna, topk=k)\n",
    "\n",
    "        if print_output == 'Y':\n",
    "        # Print the top 5 answers and their confidence scores\n",
    "            print(\"Top 5 Answers and Confidence:\")\n",
    "            print(\"1:\", answers[0]['answer'], round(answers[0]['score'], 4))\n",
    "            counter = 1\n",
    "            print(\"-------------------------------\")\n",
    "            print(\"Other Answers:\")\n",
    "            for answer in answers:\n",
    "                if counter != 1:\n",
    "                    print(f\"{counter}:\", answer['answer'], round(answer['score'], 4))\n",
    "                counter += 1\n",
    "            print()\n",
    "        \n",
    "        predicted_answers.append(answers[0]['answer'])\n",
    "    return predicted_answers  # List of predicted answers\n",
    "    \n",
    "# Call the function\n",
    "predicted_answers = answer_questions(test_questions_and_answers, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb18861c-f7dc-4343-ba1a-9810262f292e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match (EM) Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Call calculate_exact_match function \n",
    "em_score = calculate_exact_match(predicted_answers, true_answers)\n",
    "print(\"Exact Match (EM) Score:\", em_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f262cb5",
   "metadata": {},
   "source": [
    "## 4 Evaluation of selected model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbf6ee-50cd-4282-bf71-720dc3a8f854",
   "metadata": {},
   "source": [
    "I chose to use the pre-trained model as the exact match score is 0.5 as compared to the 0.17 produced by the hybrid model. This indicates that the pre-trained model produced results that are much more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f15111-e058-4a43-a3ac-c2b0c049e6c4",
   "metadata": {},
   "source": [
    "### 4.1 MRR and MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ffd7bed-9d72-4767-a774-681d196276b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank (MRR): 0.59\n",
      "Mean Average Precision (MAP): 0.6\n"
     ]
    }
   ],
   "source": [
    "# Code from ChatGPT 3.5 (OpenAI, 2022)\n",
    "    # Prompt: Write me a function to obtain MRR and MAP from my model?\n",
    "    # Relevant edits were made to suit my needs\n",
    "def calculate_mrr_and_map(test_questions_and_answers, df, top_k=5):\n",
    "    total_questions = len(test_questions_and_answers)\n",
    "    total_mrr = 0\n",
    "    total_map = 0\n",
    "\n",
    "    for i in range(total_questions):\n",
    "        # Get the ground truth answer\n",
    "        ground_truth_answer = test_questions_and_answers[i][2]\n",
    "\n",
    "        # Get the top-K answers from the model\n",
    "        qna_input = {\n",
    "            'question': test_questions_and_answers[i][1],\n",
    "            'context': preprocess(df.loc[df['id'] == test_questions_and_answers[i][0]]['article'].values[0])\n",
    "        }\n",
    "        answers = nlp_qa(qna_input, topk=top_k)\n",
    "\n",
    "        # Compute MRR\n",
    "        reciprocal_rank = 0\n",
    "        for rank, answer in enumerate(answers, 1):\n",
    "            if answer['answer'].lower() == ground_truth_answer.lower():\n",
    "                reciprocal_rank = 1 / rank\n",
    "                break\n",
    "        total_mrr += reciprocal_rank\n",
    "\n",
    "        # Compute MAP\n",
    "        precision_at_k = 0\n",
    "        relevant_answers = 0\n",
    "        for rank, answer in enumerate(answers, 1):\n",
    "            if answer['answer'].lower() == ground_truth_answer.lower():\n",
    "                relevant_answers += 1\n",
    "                precision_at_k += relevant_answers / rank\n",
    "        average_precision = precision_at_k / min(top_k, relevant_answers) if relevant_answers > 0 else 0\n",
    "        total_map += average_precision\n",
    "\n",
    "    mrr = round(total_mrr / total_questions,2)\n",
    "    map_score = round(total_map / total_questions,2)\n",
    "\n",
    "    return mrr, map_score\n",
    "\n",
    "# Call the function\n",
    "mrr_score, map_score = calculate_mrr_and_map(test_questions_and_answers, df)\n",
    "print(\"Mean Reciprocal Rank (MRR):\", mrr_score)\n",
    "print(\"Mean Average Precision (MAP):\", map_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb0d645-1aec-476a-88fd-05e8136581b4",
   "metadata": {},
   "source": [
    "### 4.2 User interaction with the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dfc19eb-6a5d-46cd-9563-c21369804dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter your quesiton: Who is the vice chairman of Samsung?\n",
      "Please enter your article number: 17574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the vice chairman of Samsung?\n",
      "Expected answer:      No expected answers \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: jay y. lee 0.9688\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: jay y. lee , 0.024\n",
      "3: the de facto leader , jay y. lee 0.002\n",
      "4: lee 0.0009\n",
      "5: , jay y. lee 0.0007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def answer_question_with_top_5(question, article_num, df, expected_answer = ''):\n",
    "    # Answer the question using the provided function\n",
    "    expected_answer = 'No expected answers'\n",
    "    answer = answer_questions([(article_num, question, expected_answer)], df, 5, \"Y\")\n",
    "\n",
    "question = str(input(\"Please enter your quesiton:\"))\n",
    "article_num = int(input(\"Please enter your article number:\"))\n",
    "\n",
    "answer_question_with_top_5(question, article_num, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc47ec",
   "metadata": {},
   "source": [
    "### 4.2 Print results of 10 test questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41a7bdae-8b68-4bf8-a418-5bb56e1ab3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is the vice chairman of Samsung?\n",
      "Expected answer:      Jay Y. Lee \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: jay y. lee 0.9688\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: jay y. lee , 0.024\n",
      "3: the de facto leader , jay y. lee 0.002\n",
      "4: lee 0.0009\n",
      "5: , jay y. lee 0.0007\n",
      "\n",
      "Question: Which subway is opening in New York City on Sunday?\n",
      "Expected answer:      Second Avenue subway \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: second avenue subway 0.4899\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: second avenue 0.2715\n",
      "3: the second avenue subway 0.2158\n",
      "4: second avenue 0.1626\n",
      "5: the second avenue 0.0716\n",
      "\n",
      "Question: What amount did Fox News offer?\n",
      "Expected answer:      20 Million \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: $ 20 million 0.8697\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: $ 20 million 0.8697\n",
      "3: 20 million 0.1175\n",
      "4: 20 million 0.1149\n",
      "5: $ 20 million offer 0.0071\n",
      "\n",
      "Question: Who is Mr. Roof's lead lawyer?\n",
      "Expected answer:      David I. Bruck \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: david i. bruck 0.9914\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: david i. bruck 0.9823\n",
      "3: david i. bruck , 0.0132\n",
      "4: bruck 0.0125\n",
      "5: standby counsel 0.0112\n",
      "\n",
      "Question: Who is the spokesman?\n",
      "Expected answer:      Numan Kurtulmus \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: numan kurtulmus 0.9853\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: aiymkan kulukeyeva 0.5567\n",
      "3: aiymkan kulukeyeva , 0.02\n",
      "4: kulukeyeva 0.0102\n",
      "5: aiymkan kulukeyeva 0.0075\n",
      "\n",
      "Question: Where is the gunman from?\n",
      "Expected answer:      Kyrgyzstan or elsewhere in Central Asia \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: kyrgyzstan 0.5884\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: kyrgyzstan 0.5789\n",
      "3: kyrgyzstan or elsewhere in central asia 0.2405\n",
      "4: kyrgyzstan or elsewhere in central asia 0.2129\n",
      "5: kyrgyzstan or elsewhere in central asia . 0.0114\n",
      "\n",
      "Question: Where is Megyn Kelly moving to from Fox News?\n",
      "Expected answer:      NBC \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: nbc 0.9927\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: nbc 0.0897\n",
      "3: nbc 0.0461\n",
      "4: nbc      0.0024\n",
      "5: nbc , 0.0017\n",
      "\n",
      "Question: What salary was Megyn Kelly offered by the Murdoch family?\n",
      "Expected answer:      More than $20 million a year \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: $ 20 million a year 0.4147\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: more than $ 20 million a year 0.3987\n",
      "3: 20 million a year 0.0998\n",
      "4: 12 year 0.0371\n",
      "5: $ 20 million 0.0078\n",
      "\n",
      "Question: How many students attend the Evergrande Football School?\n",
      "Expected answer:      2,800 students \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: 800 0.4558\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: 2 , 800 0.4517\n",
      "3: 800 student 0.036\n",
      "4: 2 , 800 student 0.0357\n",
      "5: its 2 , 800 0.0046\n",
      "\n",
      "Question: What is the main newspaper of the Communist Party in China?\n",
      "Expected answer:      People's Daily \n",
      "\n",
      "Top 5 Answers and Confidence:\n",
      "1: people daily 0.97\n",
      "-------------------------------\n",
      "Other Answers:\n",
      "2: people daily 0.9642\n",
      "3: people daily , 0.0284\n",
      "4: daily 0.0129\n",
      "5: people daily , 0.0081\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['jay y. lee',\n",
       " 'second avenue subway',\n",
       " '$ 20 million',\n",
       " 'david i. bruck',\n",
       " 'numan kurtulmus',\n",
       " 'kyrgyzstan',\n",
       " 'nbc',\n",
       " '$ 20 million a year',\n",
       " '800',\n",
       " 'people daily']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the function\n",
    "answer_questions(test_questions_and_answers[:10], df, 5, 'Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a91b4",
   "metadata": {},
   "source": [
    "## B. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb56a43-f39d-4b91-8bbb-934dc43c749a",
   "metadata": {},
   "source": [
    "1. Chan, Branden et al. (Mar. 2024). deepset/tinyroberta-squad2. https://huggingface.co/deepset/tinyroberta-squad2.\n",
    "2. Germec, M., PhD (2023) Text preprocessing with Natural Language Processing (NLP). https://www.linkedin.com/pulse/text-preprocessing-natural-language-processing-nlp-germec-phd/.\n",
    "3. OpenAI, 2022, ChatGPT, April 16, 2024, https://chat.openai.com."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7553c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## C. Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd53d80-be6f-443f-8422-389f0dd4f246",
   "metadata": {},
   "source": [
    "### C1 Building Question Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4107958f-6538-4dd4-bb3b-dcaeb0818ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the questions and answers\n",
    "# List of 100 questions for testing\n",
    "test_questions_and_answers = [\n",
    "    (17574, \"Who is the vice chairman of Samsung?\", \"Jay Y. Lee\"),\n",
    "    (17298, \"Which subway is opening in New York City on Sunday?\", \"Second Avenue subway\"),\n",
    "    (17339, 'What amount did Fox News offer?', '20 Million'),\n",
    "    (17300, 'Who is Mr. Roof\\'s lead lawyer?', 'David I. Bruck'),\n",
    "    (17314, 'Who is the spokesman?', 'Numan Kurtulmus'),\n",
    "    (17314, 'Where is the gunman from?', 'Kyrgyzstan or elsewhere in Central Asia'),\n",
    "    (17339, 'Where is Megyn Kelly moving to from Fox News?', 'NBC'),\n",
    "    (17339, 'What salary was Megyn Kelly offered by the Murdoch family?', 'More than $20 million a year'),\n",
    "    (17341, 'How many students attend the Evergrande Football School?', '2,800 students'),\n",
    "    (17341, 'What is the main newspaper of the Communist Party in China?', 'People\\'s Daily'),\n",
    "    (17341, 'What rank did the national men\\'s soccer team of China recently achieve in FIFA rankings?', '83rd'),\n",
    "    (17342, 'What is the term used to describe the biggest players in the technology industry?', 'sharks'),\n",
    "    (17342, 'Who are the \"Frightful Five\" mentioned in the article?', 'Amazon, Apple, Facebook, Microsoft, and Alphabet (Google\\'s parent company)'),\n",
    "    (17347, \"What special item did Mr. Purcell surprise Ms. Bui with?\", \"Engagement ring\"),\n",
    "    (17347, 'Which city did Mr. Purcell and Ms. Bui get engaged in?', 'San Francisco'),\n",
    "    (17354, \"What is Walter J. Clayton's background?\", \"Wall Street lawyer\"),\n",
    "    (17360, \"What accessory did the author's mother, keep in her car?\", \"A bright yellow hard hat\"),\n",
    "    (17361, \"What country was chosen as the top destination for the 2017 list?\", \"Canada\"),\n",
    "    # Add more test questions and answers here\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2b109dcd-72e2-4fc1-974a-9aec9f0e841b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 10000)\n",
    "#df[df['id'] == 17346]['article']\n",
    "\n",
    "#target_id = 17361  # Change this to the desired ID\n",
    "# Filter the DataFrame to select the article with the matching ID\n",
    "#selected_article = df[df['id'] == target_id]['article'].values[0]\n",
    "# Print the selected article\n",
    "#print(preprocess(selected_article))\n",
    "#print(\"Processed: \", preprocess(test)[:1000]) # Print first 1000 characters to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "427d810a-e57e-4187-b36d-dc819e09f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "# Define the filename\n",
    "csv_filename = \"test_questions_and_answers.csv\"\n",
    "\n",
    "# Write the data to a CSV file\n",
    "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow([\"Article ID\", \"Question\", \"Expected Answer\"])\n",
    "    # Write each row of data\n",
    "    writer.writerows(test_questions_and_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3e059",
   "metadata": {},
   "source": [
    "### C2 Extracting Entities Method\n",
    "- This method were not as effective as the method I used above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb9fbca-aed3-4db4-9742-4cc2f26ffc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract named entities from a sentence\n",
    "def extract_entities(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Function to extract the correct answer from the sentence\n",
    "def extract_answer(sentence):\n",
    "    entities = extract_entities(sentence)\n",
    "    relevant_entities = [entity for entity, label in entities if label in ['PERSON', 'ORG', 'DATE', 'MONEY']]\n",
    "    #print(relevant_entities)\n",
    "    answer = ' '.join(relevant_entities)\n",
    "    return answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
